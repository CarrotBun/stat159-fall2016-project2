## Methods  

###Standardization  
The dataset includes categories measured in different scales. To prevent any biased weighting, we want to standardize the dataset before building any models upon it.  

First, we create dummy variables for the qualitative variables discussed in the introduction. Next, we want to standardize the absolute quantitites to relative quantities (mean centering). This means that each variable will have mean zero, and standard deviation one. One reason to standardize variables is to have comparable scales. When you perform a regression analysis, the value of the computed coefficients will depend on the measurement scale of the associated predictors.  


###Regression Models  

####*Ordinary Least Squares Regression (OLS)*  
Based on the Gauss-Markov theorem, OLS is the best linear unbiased estimator. However, if predictors (regressors) are correlated, the stability of the $$\hat{beta}$$ decreases, meaning, every estimate of $$\beta$$ could be very different and not converge to the true population coefficient. 

$$Balance = \beta_1 Income + \beta_2 Limit + \beta_3 Rating + \beta_4 Cards + \beta_5 Age ... + \beta_11EthnicityCaucasian$$  

####*Ridge Regression (RR)*  
RR is a variation of the minimization in OLS Regression but with a constraint of $$$||\beta||^2_2 < c^2$$.  

In vector form: $$min \beta$$  $$||y-A\beta||^2_2 + \lambda||\beta||^2_2$$  

A difference in behavior of RR is that as $$\lambda$$ increases, more weight is given to the second term in the minimization. This means that with a large $$\lambda$$, the $$\beta$$s will be small.  

The main advantage of RR is that it takes multicollinearity into account and does automatics parameter selection. 

####*Lasso Regression (LR)*  
LR is a variation of the minimization in OLS Regression but with a constraint of $$$||\beta||_1 < c$$. With c, the constraint shape becomes a diamond and any pairs of $$\beta$$s will likely contain zeros. Unlike RR, there is no explicit form of $$\beta$$.

In vector form: $$min \beta$$  $$||y-A\beta||^2_2 + \lambda||\beta||_1$$  

The main advantage of LR is that it performs both parameter shrinkage through feature selection (sparsify regressors/predictors) and variable selection automatically. 


####*Principal Component Regression (PCR)*
PCR is based on principal component analysis. The goal of this method is to reduce the dimensions (created by the set of data points in n-dimensional space).  

To do so, we want define a direction, the first principal component, that maximizes the the variability in the data set and set the second principal component perpendicular to this first principal component. As a result, each data point's coordinates will change to this new coordinate system.  

Having dimensions with the greatest variance will maximize preservation of distances between the data points. It's important because physical distances also represent similarity. 

![A visual representation of PCA with reduction from 3 dimensions to 2 dimensions, *NLPCA*](http://www.nlpca.org/fig_pca_principal_component_analysis.png)

####*Partial Least Squares Regression (PLSR)*


##Cross Validation
